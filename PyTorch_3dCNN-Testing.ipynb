{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 3D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 33371472\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "#This downloads the model from torchvision\n",
    "model = torchvision.models.video.r3d_18(pretrained=True, progress=True)\n",
    "\n",
    "print ('Total number of parameters: {}'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# for name,param in model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# model = nn.DataParallel(model,device_ids=[2,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data for testing purposes\n",
    "torch.manual_seed(6)\n",
    "trainData = torch.rand(10, 3, 2, 4, 4)\n",
    "trainLabels = torch.randint(0, 10, (10,))\n",
    "\n",
    "testData = torch.rand(10, 3, 2, 4, 4)\n",
    "testLabels = torch.randint(0, 10, (10,))\n",
    "\n",
    "# trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "# testset_loader = DataLoader(testset, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(x, y, model, epochs=5):\n",
    "    model.train()\n",
    "    for t in range(epochs):\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(output, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print ('Epoch: {}, Loss {}'.format(t, loss))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "def check_accuracy(x, y, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    test_loss = 0\n",
    "#     model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        scores = model(x)\n",
    "        test_loss += F.cross_entropy(scores, y).item() # sum up batch loss\n",
    "        _, preds = scores.max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f), Loss: (%.2f)' % \n",
    "              (num_correct, num_samples, 100 * acc, float(test_loss) / num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Finetune) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss 2.751695394515991\n",
      "Epoch: 1, Loss 2.0147290229797363\n",
      "Epoch: 2, Loss 1.0442476272583008\n",
      "Epoch: 3, Loss 0.44752272963523865\n",
      "Epoch: 4, Loss 0.18156075477600098\n"
     ]
    }
   ],
   "source": [
    "train_model(trainData, trainLabels, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10 / 10 correct (100.00), Loss: (0.01)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(trainData, trainLabels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn            # containing various building blocks for your neural networks\n",
    "import torch.optim as optim      # implementing various optimization algorithms\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "import torchvision\n",
    "# transforms: transformations useful for image processing\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import skvideo\n",
    "from skvideo import io as vp\n",
    "\n",
    "class Cichlids(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for Cichlids.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 spatial_transform=None,\n",
    "                 preload=False):\n",
    "        \"\"\" Intialize the Cichlids dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        self.videos = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.spatial_transform = spatial_transform\n",
    "\n",
    "        # read filenames\n",
    "        for i, class_dir in enumerate(os.listdir(root)):\n",
    "#         for i in range(10):\n",
    "            filenames = glob.glob(osp.join(root, class_dir, '*.mp4'))\n",
    "            for fn in filenames:\n",
    "                self.filenames.append((fn, i)) # (filename, label) pair\n",
    "                \n",
    "        # if preload dataset into memory\n",
    "        if preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for image_fn, label in self.filenames:            \n",
    "            # load images\n",
    "            video = vp.vread(image_fn)\n",
    "#             video = video / 255\n",
    "            video = np.reshape(video, (video.shape[3], video.shape[0], video.shape[1], video.shape[2]))\n",
    "            self.videos.append(video.copy())\n",
    "            # avoid too many opened files bug\n",
    "            # image.close()\n",
    "            self.labels.append(label)\n",
    "\n",
    "    # probably the most important to customize.\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.videos is not None:\n",
    "            # If dataset is preloaded\n",
    "            video = self.videos[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            video_fn, label = self.filenames[index]\n",
    "            video = vp.vread(video_fn)\n",
    "            video = np.reshape(video, (video.shape[3], video.shape[0], video.shape[1], video.shape[2]))\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "#         if self.transform is not None:\n",
    "#             clip = [self.transform(img) for img in video]\n",
    "#         video = torch.stack(clip, 0).permute(0, 2, 1, 3)\n",
    "        \n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "            clip = [self.spatial_transform(img) for img in video]\n",
    "            video = torch.stack(clip, 0).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # return image and label\n",
    "        return video, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.ToTensor() automatically converts PIL images to\n",
    "# torch tensors with range [0, 1]\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "\n",
    "# spatial_transform = Compose([\n",
    "#             crop_method,\n",
    "#             RandomHorizontalFlip(),\n",
    "#             ToTensor(1), norm_method\n",
    "#         ])\n",
    "\n",
    "trainset = Cichlids(\n",
    "    root='MLclips/training',\n",
    "    preload=False, transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "# We want the dataset to be shuffled during training.\n",
    "trainset_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "# Load the testset\n",
    "testset = Cichlids(\n",
    "    root='MLclips/testing',\n",
    "    preload=False, transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "testset_loader = DataLoader(testset, batch_size=10, shuffle=False, num_workers=1)\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device1 = torch.device(\"cuda: 2\" if use_cuda else \"cpu\")\n",
    "device2 = torch.device(\"cuda: 4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "def train_model(epochs=5, log_interval=1):\n",
    "    iteration = 0\n",
    "    torch.cuda.empty_cache()\n",
    "#     model = nn.DataParallel(model)\n",
    "    model.to(device1)\n",
    "    model.train()\n",
    "    for t in range(epochs):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            \n",
    "            data, target = data.to(device1), target.to(device1)\n",
    "            \n",
    "            data = data.float()\n",
    "\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = F.cross_entropy(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    t, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "            \n",
    "            break\n",
    "        end = time()\n",
    "#         print('Time taken for this epoch: {:.2f}s'.format(end-start))\n",
    "        check_accuracy(data, target, model) # evaluate at the end of epoch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def check_accuracy(x, y, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    test_loss = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        scores = model(x)\n",
    "        test_loss += F.cross_entropy(scores, y).item() # sum up batch loss\n",
    "        _, preds = scores.max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f), Loss: (%.2f)' % \n",
    "              (num_correct, num_samples, 100 * acc, float(test_loss) / num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/11419 (0%)]\tLoss: 1.904350\n",
      "Got 0 / 1 correct (0.00), Loss: (2.28)\n",
      "Train Epoch: 1 [0/11419 (0%)]\tLoss: 2.102967\n",
      "Got 1 / 1 correct (100.00), Loss: (0.00)\n",
      "Train Epoch: 2 [0/11419 (0%)]\tLoss: 0.002038\n",
      "Got 1 / 1 correct (100.00), Loss: (0.00)\n",
      "Train Epoch: 3 [0/11419 (0%)]\tLoss: 0.000000\n",
      "Got 1 / 1 correct (100.00), Loss: (0.00)\n",
      "Train Epoch: 4 [0/11419 (0%)]\tLoss: 56.559669\n",
      "Got 0 / 1 correct (0.00), Loss: (2.50)\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "from skvideo import io as vp\n",
    "\n",
    "vdata = vp.vread('MLclips/training/b/1025_4473_3982_439_811.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n",
      "tensor([0, 0, 0])\n",
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for idx, (data, labels) in enumerate(trainset_loader):\n",
    "        print (labels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 200, 200, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_17 (Conv3D)           (None, 118, 198, 198, 32) 2624      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 118, 99, 99, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 116, 97, 97, 64)   55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 116, 48, 48, 64)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 114, 46, 46, 128)  221312    \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 112, 44, 44, 128)  442496    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 112, 22, 22, 128)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 111, 21, 21, 256)  262400    \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 110, 20, 20, 256)  524544    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 110, 10, 10, 256)  0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2816000)           0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              2883585024\n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,886,153,610\n",
      "Trainable params: 2,886,153,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu', input_shape=(120, 200, 200, 3)))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(64, (3,3,3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "model.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
